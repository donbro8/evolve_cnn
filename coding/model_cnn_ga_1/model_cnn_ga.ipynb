{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAS for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Branches\n",
    "- Connections\n",
    "- Layers\n",
    "    - Input\n",
    "        - Input shape (first input is standard, all others are calculated)\n",
    "        - Batch size \n",
    "    - 2D convolutional\n",
    "        - Filters\n",
    "        - Kernel size\n",
    "        - Padding\n",
    "        - Activation\n",
    "    - Pooling\n",
    "        - Max\n",
    "            - Size\n",
    "            - Strides\n",
    "            - Padding\n",
    "        - Average\n",
    "            - Size\n",
    "            - Strides\n",
    "            - Padding\n",
    "    - Dropout\n",
    "        - Rate\n",
    "    - Batch normalisation\n",
    "    - Concatenate\n",
    "        - Axis\n",
    "    - Flatten\n",
    "    - Dense\n",
    "        - Units (1, 2)\n",
    "        - Activation (sigmoid, softmax)\n",
    "    - Global pooling\n",
    "        - Max\n",
    "        - Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GA\n",
    "    - Type\n",
    "    - Number of generations\n",
    "- Population\n",
    "    - Size\n",
    "    - Encoding type\n",
    "    - Individual size\n",
    "        - Variable or fixed\n",
    "- Fitness\n",
    "- Selection\n",
    "- Reproduction\n",
    "- Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.spatial import distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'input':{\n",
    "        'batch':[8, 16, 32, 64, 128, 256]\n",
    "        },\n",
    "    'conv':{\n",
    "        'kernel':['1x1', '3x3', '5x5', '7x7'],\n",
    "        'filter':[2, 4, 8, 16, 32, 64, 128],\n",
    "        'padding':['valid', 'same'],\n",
    "        'activation':['tanh', 'relu', 'selu', 'elu']\n",
    "        },\n",
    "    'pool':{\n",
    "        'type':['max', 'average'],\n",
    "        'size':['2x2', '3x3', '4x4', '5x5'],\n",
    "        'padding':['valid', 'same']\n",
    "        },\n",
    "    'dropout':{\n",
    "        'type':['dropout','spatial2D'],\n",
    "        'rate':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        },\n",
    "    'output':{\n",
    "        'type':['global', 'dense']\n",
    "        }\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 16, 32, 64, 128, 256]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual = [\n",
    "    # Input layer\n",
    "    'batch_size_index', \n",
    "    \n",
    "    # Convolutional layer\n",
    "    'kernel_index', 'filter_index', 'padding_index', 'activation_index',\n",
    "\n",
    "    # Pooling layer\n",
    "    'type_index', 'size_index', 'padding_index',\n",
    "\n",
    "    # Dropout layer\n",
    "    'type_index', 'rate_index',\n",
    "\n",
    "    # Output layer\n",
    "    'type_index',\n",
    "\n",
    "    # Add layer\n",
    "    ## Layer/Branch 1\n",
    "    'indicator', 'branch_indicator', 'type', 'from', 'to',\n",
    "\n",
    "    ## Layer/Branch 2\n",
    "    'indicator', 'branch_indicator', 'type', 'from', 'to',\n",
    "\n",
    "    ## ... variable length array\n",
    "\n",
    "    ## Layer/Branch n\n",
    "    'indicator', 'branch_indicator', 'type', 'from', 'to'\n",
    "\n",
    "\n",
    "    ]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_space(parameters):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the encoding length of an individual based on the input parameters dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    len_lists = []\n",
    "    len_values = [] # stores the maximum lengths of bits required to define the length of the lists in the parameters dict\n",
    "    max_ind = []\n",
    "    bit_max_int = []\n",
    "    bit_max = ''\n",
    "\n",
    "    for layer in params.keys():\n",
    "\n",
    "            for value_list in list(params[layer].values()):\n",
    "\n",
    "                max_ind.append(len(value_list) - 1)\n",
    "\n",
    "                bit_max = bit_max + bin(len(value_list) - 1)[2:]\n",
    "\n",
    "                len_values.append(len(bin(len(value_list) - 1)[2:])) \n",
    "\n",
    "                len_lists.append(len(value_list))\n",
    "        \n",
    "\n",
    "    gene_length = np.max(len_values)\n",
    "    \n",
    "    individual_length = len(len_lists) * gene_length\n",
    "\n",
    "    len_values = np.array(len_values)\n",
    "\n",
    "    len_lists = np.array(len_lists)\n",
    "\n",
    "    return individual_length, gene_length, len_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 4 [6 4 7 2 4 2 4 2 2 9 2]\n"
     ]
    }
   ],
   "source": [
    "ind_len, gene_len, len_list = get_state_space(params)\n",
    "print(ind_len, gene_len, len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_population(length, parameter_len, n = 10, seed = None):\n",
    "    \"\"\"\n",
    "    Generate population given the number of individuals in a population and the the required binary length\n",
    "    \"\"\"\n",
    "\n",
    "    bit_length = int(length/len(parameter_len))\n",
    "\n",
    "    if seed != None:\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    population = np.zeros(shape=(n, length), dtype=int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        bits = ''\n",
    "        for l in parameter_len:\n",
    "\n",
    "            choice = np.random.choice(np.arange(0, l))\n",
    "\n",
    "            bit_choice = bin(choice)[2:]\n",
    "\n",
    "            bits += ('0' * (bit_length - len(bit_choice))) + bit_choice\n",
    "\n",
    "        population[i] = np.array([int(x) for x in bits])\n",
    "    \n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "pop = generate_population(ind_len, len_list, n = 100, seed = 43)\n",
    "print(pop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(chromosome, gene_length, parameter_lengths):\n",
    "\n",
    "    \"\"\"\n",
    "    Converts the binary bitstring vector to integer phenotype and performs check to see if it is valid with respect to the constraints\n",
    "    \"\"\"\n",
    "\n",
    "    chromo_length = len(chromosome)\n",
    "\n",
    "    start_ind = np.arange(0, chromo_length, gene_length)\n",
    "\n",
    "    phenotype = np.array([int(str(''.join(map(str, chromosome[x:x + gene_length]))), 2) for x in start_ind])\n",
    "\n",
    "    is_valid = np.any(phenotype < parameter_lengths - 1)\n",
    "\n",
    "    return phenotype, is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 6 0 3 0 0 0 1 2 0] True\n"
     ]
    }
   ],
   "source": [
    "pheno, valid = decoder(pop[0], gene_len, len_list)\n",
    "print(pheno, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(population, selection_probability = 0.1, seed = None):\n",
    "\n",
    "    n = population.shape[0]\n",
    "\n",
    "    n_select = int(np.ceil(n*selection_probability))\n",
    "\n",
    "    mean_hamming = 0\n",
    "\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    for individual in np.random.choice(np.arange(0, n, n_select), n_select):\n",
    "\n",
    "        mean_hamming += np.mean([distance.hamming(population[individual], population[x]) for x in range(n)])\n",
    "\n",
    "    return mean_hamming/n_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23049999999999998"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_distance(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(fitness_input, population = None, model_params = None):\n",
    "\n",
    "    diversity = 1\n",
    "\n",
    "    n_params = 1\n",
    "\n",
    "    if population != None:\n",
    "        diversity = hamming_distance(population)\n",
    "\n",
    "    if model_params != None:\n",
    "        n_params = 1 # need to determine the maximum number of possible parameters so that we can scale\n",
    "\n",
    "    return 1/fitness_input * 1/diversity * n_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(population, k = 2, selection_probability = 0.9, seed = None):\n",
    "\n",
    "    n = population.shape[0]\n",
    "\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    sub_population = np.random.choice(np.arange(0, n, 1), k, replace = False)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in sub_population:\n",
    "\n",
    "        # train model\n",
    "\n",
    "        scores.append(fitness(np.random.random()))\n",
    "\n",
    "    rank_index = np.argsort(scores)\n",
    "\n",
    "    rank_scores = np.array(scores)[rank_index]\n",
    "\n",
    "    ranked = sub_population[rank_index]\n",
    "\n",
    "    p_array = np.concatenate(([selection_probability], selection_probability*((1 - selection_probability)**rank_scores)))\n",
    "\n",
    "    p_array = np.concatenate((p_array[p_array < 1], np.array([1 - np.sum(p_array[p_array < 1])])))\n",
    "\n",
    "    p_array = np.concatenate((p_array[0:len(ranked) - 1], [np.sum(p_array[len(ranked) - 1:])]))\n",
    "\n",
    "\n",
    "    return np.random.choice(ranked, p = p_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96 22]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3647436c2f734fd3f72be8f73130b09ecd5d6c908b22991734a3f7b2100497cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
